{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23c3155a",
   "metadata": {},
   "source": [
    "# Trend Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72482e79",
   "metadata": {},
   "source": [
    "## Miners\n",
    "\n",
    "In this notebook you will be able to mine **Reddit**, **Scopus**, and **Stackoverflow**. \n",
    "- Configurations for this notebook can be found in **Miners.yaml** file inside the **Config** folder\n",
    "- Make sure you follow the setup instructions on **Readme.md** and have installed all the packages required for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae4ff5",
   "metadata": {},
   "source": [
    "\n",
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a291336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import yaml\n",
    "import praw  \n",
    "import requests\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pybliometrics \n",
    "from yaspin import yaspin\n",
    "from datetime import datetime   \n",
    "from yaml.loader import SafeLoader \n",
    "from pybliometrics.scopus import ScopusSearch\n",
    "from pybliometrics.scopus.utils import config \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43637d1",
   "metadata": {},
   "source": [
    "### Load Config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9edd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Config/Miners.yaml') as f:\n",
    "    config = yaml.load(f, Loader=SafeLoader)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ae13db",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createFile(file, path): \n",
    "    \"\"\"This function is used to create the directory necessary to store the mined data.        \n",
    "\n",
    "    Args:\n",
    "        file (str): Name of the file to be created.\n",
    "        path (str): Path of the directory where the files will be stored e.g. \"Data\".\n",
    "    \"\"\"\n",
    "    complete_path = f'{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{path}'\n",
    "    does_folder_exist = os.path.exists(complete_path)\n",
    "    does_file_exist  = os.path.exists(f'{complete_path}\\\\{file}')\n",
    "    if (does_folder_exist): \n",
    "        # Remove existing stack data file if already exist to add new one\n",
    "        if (does_file_exist):\n",
    "            print('Removing already existing',file,'file')\n",
    "            os.remove(f'{complete_path}\\\\{file}')\n",
    "        else:\n",
    "            print( file + ' does not exist yet, ' + 'it will be downloaded')\n",
    "\n",
    "    # Create Data folder if did not exist to store the csv file\n",
    "    else: \n",
    "        root_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "        os.mkdir(f'{root_dir}\\\\{path}')\n",
    "        print(f'{path} folder created for csv file storage')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ffcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveFile(file, filename, path):\n",
    "    complete_path = f'{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{path}'\n",
    "    file.to_csv(f'{complete_path}\\\\{filename}')\n",
    "    print(f'{filename} saved in {path} directory')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2522195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file, path):\n",
    "    try:\n",
    "        spinner = yaspin()\n",
    "        complete_path = f'{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{path}\\\\{file}'\n",
    "        file_data = pd.read_csv(complete_path, index_col=0)\n",
    "        spinner.write(\"‚úîÔ∏è File loaded.\")\n",
    "        spinner.stop()\n",
    "        return file_data\n",
    "    except Exception as e:\n",
    "        print('Error reading file',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b343eb7",
   "metadata": {},
   "source": [
    "### Reddit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49ea289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(subreddit, reddit_client_id, reddit_client_secret, reddit_user_agent):\n",
    "    \"\"\"This function mines the data from subreddits\n",
    "\n",
    "    Args:\n",
    "        subreddit (str): Name of the subreddit to be mined\n",
    "        reddit_client_id (str) : Client ID\n",
    "        reddit_client_secret (str) : Client Secret\n",
    "        reddit_user_agent (str) : User agent\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        reddit: praw.reddit.Reddit = praw.Reddit(client_id=reddit_client_id,client_secret=reddit_client_secret,user_agent=reddit_user_agent,check_for_async=False)\n",
    "        subreddit: praw.models.reddit.subreddit.Subreddit = reddit.subreddit(subreddit) \n",
    "\n",
    "        spinner = yaspin()\n",
    "        spinner.start()\n",
    "        spinner.write( f\"‚õèÔ∏è Mining {subreddit} subreddit\")\n",
    "        \n",
    "\n",
    "        posts= []\n",
    "        columns=['AuthorId', 'Q_id', 'Title', 'Abstract', 'Answers', 'Cites',  'Date']\n",
    "\n",
    "        for post in subreddit.hot(limit=None):\n",
    "            posts.append([post.author, post.id, post.title, \n",
    "                      post.selftext, post.num_comments, post.score,\n",
    "                       datetime.fromtimestamp( post.created) \n",
    "                      ])\n",
    "\n",
    "        spinner.ok(\"‚úîÔ∏è Data Mined\")\n",
    "        reddit_data: pandas.core.frame.DataFrame = pd.DataFrame(posts,columns=columns)\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to mine\")\n",
    "        spinner.write(e)    \n",
    "    finally:\n",
    "        spinner.stop()\n",
    "      \n",
    "    try:\n",
    "        createFile(config['REDDIT_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        saveFile(reddit_data, config['REDDIT_DATA_CSV'], config['STORAGE_PATH'] )\n",
    "        spinner.ok(\"‚úîÔ∏è File saved\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to save file\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "getData(config['REDDIT_SUBREDDIT'], config['REDDIT_CLIENT_ID'], config['REDDIT_CLIENT_SECRET'], config['REDDIT_USER_AGENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b91b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reddit_cleaner(data):\n",
    "    \"\"\"This function is applied to the dataframe, it removes the unnecessary characters  and symbols from it\n",
    "\n",
    "    Args:\n",
    "        data (string): Data string that needs to be cleaned\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned string \n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = str(data)  \n",
    "        res = re.sub('\\[[^]]*\\]' , '', data) #remove eveything in []\n",
    "        res = re.sub(\"<a.*?>*</a>\" , '', data) #remove anchor tags with content\n",
    "        res = re.sub(\"[0-9]\" , '', res) #remove numbers\n",
    "        res = re.sub(\"&quot\", '', res) #remove &quot\n",
    "        res = re.sub(\"<.*?>\", '', res) #remove all HTML tags\n",
    "        res = re.sub(\"//.*\\n\", '', res)\n",
    "        res = re.sub(\"\\\\{\\n.*\\\\}\\n\", '', res)\n",
    "        res = re.sub(\"[\\r\\n]\", '', res)\n",
    "        res = re.sub(\"\\\"\", '', res) #remove quotes\n",
    "        res = re.sub('[^\\w\\s]', ' ', res) #remove punctuations\n",
    "        res = res.lower()\n",
    "        return res\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error cleaning data',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160414f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reddit_data():\n",
    "    \"\"\"This function cleans the dataframes by applying the clean_data function to each title and abstract in the dataframe\n",
    "        Also it drops the row if it has no date and if its abstract is missing\n",
    "    \"\"\"\n",
    "    spinner = yaspin()\n",
    "    spinner.start()\n",
    "    \n",
    "    try:\n",
    "        spinner.write('üßπ Data cleaning')\n",
    "        reddit_data = readFile(config['REDDIT_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        reddit_data['Title_clean'] = reddit_data['Title'].apply(reddit_cleaner)\n",
    "        abstract = reddit_data.Abstract\n",
    "        cleaned_abstract = abstract.apply(reddit_cleaner)\n",
    "        reddit_data['Abstract_clean'] = cleaned_abstract\n",
    "\n",
    "        # Drop the rows which have empty abstract\n",
    "        reddit_data.drop(reddit_data[reddit_data['Abstract'] == ''].index, inplace=True)\n",
    "\n",
    "        # Drop rows with no date\n",
    "        reddit_data.drop(reddit_data[(reddit_data['Date'] == '') | \n",
    "                               (reddit_data['Date'] == None) |\n",
    "                               (reddit_data['Date'] == 0) ].index, \n",
    "                               inplace=True\n",
    "                                )\n",
    "    \n",
    "        # Drop null rows \n",
    "        reddit_data.dropna(axis=0, inplace=True)\n",
    "        spinner.ok(\"‚úîÔ∏è Data cleaned\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to clean data\")  \n",
    "        spinner.write(e)\n",
    "        print(e)\n",
    "    finally:\n",
    "        spinner.stop()\n",
    "        \n",
    "    try:\n",
    "        spinner.write(\"üîÅ Old file will be replaced\\n\")\n",
    "        createFile(config['REDDIT_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        saveFile(reddit_data, config['REDDIT_DATA_CSV'], config['STORAGE_PATH'] )\n",
    "        spinner.ok(\"‚úîÔ∏è File saved\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to save file\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999ad306",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reddit_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8e6a4",
   "metadata": {},
   "source": [
    "### Stackoverflow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaedf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotal(stk_query_string,stackoverflow_api_key ):\n",
    "    \"\"\"This function gets the total number of results in response\n",
    "\n",
    "    Args:\n",
    "        stk_query_string (str): query string\n",
    "    \"\"\"\n",
    "    spinner = yaspin()\n",
    "    spinner.start()\n",
    "    try:\n",
    "        spinner.write('Fetching total')\n",
    "        total_api_url =  f'https://api.stackexchange.com/2.2/search/advanced?order=desc&sort=activity&q={stk_query_string}&filter=total&site=stackoverflow&key={stackoverflow_api_key}'\n",
    "        res =  requests.get(total_api_url)\n",
    "        res = res.json()\n",
    "        total_num = res['total']\n",
    "        spinner.write(f'‚úîÔ∏è total: {total_num}')\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to get total\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f756ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "getTotal(config['STACKOVERFLOW_QUERY_STRING'], config['STACKOVERFLOW_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c388cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(query, page_number, stackoverflow_api_key ):\n",
    "    \"\"\"This function is used to fetch data.\n",
    "\n",
    "    Args:\n",
    "        query (str): query string.   \n",
    "        page_number (int): page number to be mined.\n",
    "        stackoverflow_api_key (str): api key\n",
    "    Returns:\n",
    "        pd.DataFrame: response of the API stored in the pandas data frame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f'https://api.stackexchange.com/2.2/search/advanced?order=desc&sort=activity&q={query}&filter=withbody&site=stackoverflow&key={stackoverflow_api_key}&page={page_number}'\n",
    "        res =  requests.get(url)\n",
    "        res = res.json() \n",
    "        return pd.DataFrame(res)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177003b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBody():\n",
    "    \"\"\"This function mines Stackoverflow.\n",
    "\n",
    "    Args:\n",
    "        stk_query_string (str): query string to be searched, mined and saved in a CSV.\n",
    "    \"\"\"\n",
    "    spinner = yaspin()\n",
    "    spinner.start()\n",
    "    try:\n",
    "        spinner.write('‚õèÔ∏è Mining Stackoverflow')\n",
    "        \n",
    "        page_number = 1 \n",
    "        df = fetch_data(config['STACKOVERFLOW_QUERY_STRING'], page_number, config['STACKOVERFLOW_API_KEY'])\n",
    "\n",
    "        while df.iloc[-1]['has_more']:\n",
    "            page_number = page_number + 1\n",
    "            fetched_data = fetch_data(config['STACKOVERFLOW_QUERY_STRING'], page_number, config['STACKOVERFLOW_API_KEY'])\n",
    "            df = pd.concat([df, fetched_data], ignore_index=True) \n",
    "\n",
    "            if not fetched_data.iloc[-1]['has_more']:\n",
    "                spinner.write(f'‚ÑπÔ∏è Data fetch completed with {len(df)} records')\n",
    "                break\n",
    "\n",
    "        # Organize Data\n",
    "        spinner.write('üóÉÔ∏è Organizing data')\n",
    "        user_data = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            user = {}\n",
    "            user['AuthorId'] = row['items']['owner'].get('user_id',0)\n",
    "            user['Q_id'] = row['items'].get('question_id', '') \n",
    "            user['Title'] = row['items'].get('title', '')\n",
    "            user['Abstract'] = row['items'].get('body', '') \n",
    "            user['Views'] = row['items'].get('view_count', 0) \n",
    "            user['Answers'] = row['items'].get('answer_count', 0)  \n",
    "            user['Cites'] = row['items'].get('score', 0) \n",
    "            user['Tags_n'] = len(row['items'].get('tags', []))  \n",
    "            user['Tags'] = ';'.join(row['items'].get('tags', ''))\n",
    "            user['Date'] =  datetime.fromtimestamp( row['items']['creation_date']) \n",
    "            user['CR_Date'] =  datetime.fromtimestamp( row['items']['creation_date']) \n",
    "            user['LA_Date'] =  datetime.fromtimestamp( row['items']['last_activity_date'])   \n",
    "\n",
    "            user_data.append(user) \n",
    "\n",
    "        stack_data = pd.DataFrame(data=user_data)\n",
    "        spinner.ok(\"‚úîÔ∏è Data Mined\") \n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to mine stackoverflow\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()\n",
    "        \n",
    "    try: \n",
    "        createFile(config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        saveFile(stack_data, config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'] )\n",
    "        spinner.ok(\"‚úîÔ∏è File saved\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to save file\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "getBody()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stackoverflow_cleaner(data,is_abstract):\n",
    "    \"\"\"This function is applied to the dataframe, it removes the unnecessary characters  and symbols from it.\n",
    "\n",
    "    Args:\n",
    "        data (str): data to be cleaned\n",
    "        is_abstract (bool): flag to indicate if this function is applied on abstract or title\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned data\n",
    "    \"\"\"\n",
    "    data = str(data)  \n",
    "    if is_abstract:\n",
    "        reg_str = \"<p>(.*?)</p>\" #get only text for abastracts\n",
    "        res = re.findall(reg_str, data)\n",
    "        res = ' '.join(res)\n",
    "    else:\n",
    "        res = data\n",
    "\n",
    "    res = re.sub(\"<a.*?>*</a>\" , '', res) #remove anchor tags with content\n",
    "    res = re.sub(\"[0-9]\" , '', res) #remove numbers\n",
    "    res = re.sub(\"&quot\", '', res) #remove &quot\n",
    "    res = re.sub(\"<.*?>\", '', res) #remove all HTML tags\n",
    "    res = re.sub(\"//.*\\n\", '', res)\n",
    "    res = re.sub(\"\\\\{\\n.*\\\\}\\n\", '', res)\n",
    "    res = re.sub(\"[\\r\\n]\", '', res)\n",
    "    res = re.sub(\"\\\"\", '', res) #remove quotes\n",
    "    res = re.sub('[^\\w\\s]', ' ', res) #remove punctuations\n",
    "    res = res.lower()\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260acb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData():\n",
    "    \"\"\"\n",
    "    This function cleans the dataframes by applying the clean function to each abstract in the dataframe. \n",
    "    In this function data points has been droped where abstract and date is missing \n",
    "    \"\"\"\n",
    "    spinner = yaspin()\n",
    "    spinner.start()\n",
    "    try:\n",
    "        \n",
    "        spinner.write('üßπ Data cleaning')\n",
    "        stack_data =  readFile(config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        abstract = stack_data.Abstract\n",
    "        title = stack_data.Title\n",
    "        cleaned_abstract = abstract.apply(stackoverflow_cleaner, is_abstract=True)\n",
    "        cleaned_title = title.apply(stackoverflow_cleaner, is_abstract=False)\n",
    "        stack_data['Abstract_clean'] = cleaned_abstract\n",
    "        stack_data['Title_clean'] = cleaned_title\n",
    "        #Drop rows where abstract has empty value\n",
    "        stack_data.drop(stack_data[stack_data['Abstract'] == ''].index, inplace=True)\n",
    "        stack_data.drop(stack_data[stack_data['Abstract_clean'] == ''].index, inplace=True)\n",
    "\n",
    "        #Drop rows with no date\n",
    "        stack_data.drop(stack_data[(stack_data['Date'] == '') | (stack_data['Date'] == None) | (stack_data['Date'] == 0) ].index, inplace=True)\n",
    "        # Drop null rows\n",
    "         \n",
    "        stack_data.dropna(axis=0, inplace=True, how=\"any\")\n",
    "        spinner.ok(\"‚úîÔ∏è Data cleaned\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to clean data\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()\n",
    "    \n",
    "    try:\n",
    "        spinner.write(\"üîÅ Old file will be replaced\\n\")\n",
    "        createFile(config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        saveFile(stack_data, config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'] )\n",
    "        spinner.ok(\"‚úîÔ∏è File saved\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to save file\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363c77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a32d2",
   "metadata": {},
   "source": [
    "### Scopus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b566cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîë Please enter the following key in the input \\n\")\n",
    "print(config['SCOPUS_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34199f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pybliometrics.scopus.utils.create_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea89772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(query):\n",
    "    \"\"\"This function mines the Scopus database\n",
    "\n",
    "    Args:\n",
    "        query (str): query or string that will be used as a criteria while mining\n",
    "    \"\"\"\n",
    "    spinner = yaspin()\n",
    "    spinner.start()\n",
    "    try:\n",
    "        spinner.write('‚õèÔ∏è Mining scopus')\n",
    "        scopus_query = query\n",
    "        scopus_res = ScopusSearch(scopus_query,  download=True, view='COMPLETE')\n",
    "        spinner.write(f'‚ÑπÔ∏è Total entries {scopus_res.get_results_size()}' ) \n",
    "\n",
    "        scopus_data = pd.DataFrame(pd.DataFrame(scopus_res.results))\n",
    "        spinner.write(f'‚ÑπÔ∏è Dataframe shape {scopus_data.shape}' )\n",
    "\n",
    "        scopus_data_subset = scopus_data[['eid', 'doi', 'title', 'creator', 'publicationName', 'coverDate', 'description', \n",
    "                           'authkeywords', 'citedby_count', 'pageRange', 'aggregationType', 'subtypeDescription',\n",
    "                          'author_count', 'author_names', 'author_ids', 'affilname', 'affiliation_country'\n",
    "                          ]]\n",
    "        spinner.ok(\"‚úîÔ∏è Data Mined\") \n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to mine scopus\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()\n",
    "    \n",
    "    try:\n",
    "        createFile(config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        saveFile(scopus_data_subset, config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'] )\n",
    "        spinner.ok(\"‚úîÔ∏è File saved\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to save file\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sco_query = f\"TITLE-ABS-KEY(\\ '{config['SCOPUS_QUERY_STRING_1']}' \\) AND ALL ( '{config['SCOPUS_QUERY_STRING_2']}')\"\n",
    "# sco_query = \"TITLE(config['SCOPUS_QUERY_STRING_1']) AND PUBYEAR > 2021\"\n",
    "getData(sco_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac52e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scopus_cleaner(data):\n",
    "    \"\"\"This function is applied to the dataframe, it removes the unnecessary characters  and symbols from it\n",
    "\n",
    "    Args:\n",
    "        data (string): Data string that needs to be cleaned\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned string \n",
    "    \"\"\"\n",
    "    data = str(data)\n",
    "    res = re.sub(\"[¬©¬Æ‚Ñ¢%]\", \"\", data) #remove ¬©,¬Æ,‚Ñ¢,% sign \n",
    "    res = re.sub(\"<a.*?>*</a>\", '', data) #remove anchor tags with content\n",
    "    res = re.sub(\"[0-9]\", '', res) #remove numbers\n",
    "    res = re.sub(\"<.*?>\", '', res) #remove all HTML tags\n",
    "    res = re.sub(\"//.*\\n\", '', res)\n",
    "    res = re.sub(\"\\\\{\\n.*\\\\}\\n\", '', res)\n",
    "    res = re.sub(\"[\\r\\n]\", '', res)\n",
    "    res = re.sub(\"\\\"\", '', res) #remove quotes\n",
    "    res = re.sub('[^\\w\\s]', ' ', res) #remove punctuations\n",
    "    res = re.sub(\"All right reserved[.]\", ' ', res) #\n",
    "    res = data.lower()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean():\n",
    "    \"\"\"\n",
    "    This function cleans the dataframes by applying the clean_scopus_data function to each abstract in the dataframe\n",
    "        Also it renames few important column names  \n",
    "    \"\"\"\n",
    "    spinner = yaspin()\n",
    "    spinner.start()\n",
    "    try:\n",
    "        spinner.write('üßπ Data cleaning')\n",
    "        scopus_data_subset = readFile(config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        abstract = scopus_data_subset['description']\n",
    "        title = scopus_data_subset['title']\n",
    "        cleaned_abstract = abstract.apply(scopus_cleaner)\n",
    "        cleaned_title = title.apply(scopus_cleaner)\n",
    "        scopus_data_subset['Abstract_clean'] = cleaned_abstract \n",
    "        scopus_data_subset['Title_clean'] = cleaned_title\n",
    "        scopus_data_subset.dropna(axis=0, inplace=True)\n",
    "        scopus_data_subset.rename(columns={'description':'Abstract', 'coverDate': 'Date', 'citedby_count': 'Cites', 'title': 'Title'}, inplace=True)\n",
    "        spinner.ok(\"‚úîÔ∏è Data cleaned\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to clean data\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()\n",
    "         \n",
    "        \n",
    "    try:\n",
    "        spinner.write(\"üîÅ Old file will be replaced\\n\")\n",
    "        createFile(config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'])\n",
    "        saveFile(scopus_data_subset, config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'] )\n",
    "        spinner.ok(\"‚úîÔ∏è File saved\")\n",
    "    except Exception as e:\n",
    "        spinner.fail(\"‚ùå Failed to save file\")  \n",
    "        spinner.write(e)\n",
    "    finally:\n",
    "        spinner.stop()\n",
    "\n",
    "        # TODO: Remove papers that are summaries of conference proceedings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8624dbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
