{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef75712",
   "metadata": {},
   "source": [
    "# Trend Mining "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e614fa",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "In this notebook you will be able to train and build LDA model. \n",
    "- Configurations for this notebook can be found in **LDA.yaml** file inside the **Config** folder\n",
    "- Make sure you follow the setup instructions on **Readme.md** and have installed all the packages required for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524c1ef8",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe417ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis.sklearn\n",
    "from yaspin import yaspin\n",
    "from ast import literal_eval\n",
    "import statsmodels.api as sma\n",
    "from collections import Counter\n",
    "from yaml.loader import SafeLoader\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings('ignore')\n",
    "# !python -m spacy download en # Run this for first time after installation you can comment this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c32c2f",
   "metadata": {},
   "source": [
    "### Load Config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Config/Miners.yaml') as f:\n",
    "    config = yaml.load(f, Loader=SafeLoader)\n",
    "print('General Config:', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d26f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Config/LDA.yaml') as f:\n",
    "    LDAConfig = yaml.load(f, Loader=SafeLoader)\n",
    "print('LDA Config:', LDAConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b7fcf",
   "metadata": {},
   "source": [
    "### Common function and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file, path):\n",
    "    try:\n",
    "        spinner = yaspin()\n",
    "        complete_path = f'{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{path}\\\\{file}'\n",
    "        file_data = pd.read_csv(complete_path, index_col=0)\n",
    "        spinner.write(\"‚úîÔ∏è File loaded.\")\n",
    "        spinner.stop()\n",
    "        return file_data\n",
    "    except Exception as e:\n",
    "        print('Error reading file',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad62e92",
   "metadata": {},
   "source": [
    "#### Common class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb61a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    \"\"\"This is the class implementation for calculating LDA\n",
    "    \"\"\"\n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame \n",
    "        self.dirName = \"\"\n",
    "        self.tokenized = \"\"\n",
    "        self.lemmatized = \"\"\n",
    "        self.vectorizer = \"\"\n",
    "        self.vectorized = \"\"\n",
    "        self.lda_model = \"\"\n",
    "        self.best_lda_model = \"\"\n",
    "        self.best_model_output = \"\"\n",
    "        self.df_document_topic = \"\"\n",
    "        self.df_topic_distribution = \"\"\n",
    "        self.df_topic_keywords = \"\"\n",
    "        self.hot = \"\"\n",
    "        self.cold =\"\"\n",
    "        self.spinner = yaspin()\n",
    "    \n",
    "    \n",
    "\n",
    "    def createOutputDir(self, dirName):\n",
    "        \"\"\"This function creates the folder to store the output graphs and images\n",
    "\n",
    "        Args:\n",
    "            dirName (str): Name of the output folder\n",
    "        \"\"\"\n",
    "        self.dirName = dirName\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\LDA\\\\{self.dirName}\"\n",
    "        does_folder_exist = os.path.exists(complete_path)\n",
    "        if (does_folder_exist):\n",
    "            self.spinner.write(\"‚úîÔ∏è Output directory already exists.\")\n",
    "        else:\n",
    "            os.makedirs(complete_path)\n",
    "            self.spinner.write(\"‚úîÔ∏è Folder created for output storage\")\n",
    "\n",
    "    def mergeTokenizedData(self):\n",
    "        \"\"\"This function converts the string representation of tokenized data into list\n",
    "        \"\"\"\n",
    "        tokenized_rows = []\n",
    "        for index, row in self.data_frame.iterrows(): \n",
    "            tokenized_rows.append(literal_eval(row[\"Tokenized_data\"]))\n",
    "        self.tokenized = tokenized_rows\n",
    "        self.spinner.write(\"‚úîÔ∏è Tokenized data merged\")\n",
    "    \n",
    "    def lemmatization(self, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"This function is used to lemmitize the tokenized data\n",
    "\n",
    "        Args:\n",
    "            allowed_postags (list, optional): Allowed postags for lemmitization. Defaults to ['NOUN', 'ADJ', 'VERB', 'ADV'].\n",
    "        \"\"\"\n",
    "        nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        texts_out = []\n",
    "        for sent in self.tokenized:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
    "        self.lemmatized = texts_out\n",
    "        self.spinner.write(\"‚úîÔ∏è Lemmitization applied.\")\n",
    "        \n",
    "    def vectorization(self):\n",
    "        \"\"\"This function is used to vectorize the lemmitized data\n",
    "        \"\"\"\n",
    "        self.vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "        self.vectorized = self.vectorizer.fit_transform(self.lemmatized)\n",
    "        self.spinner.write(\"‚úîÔ∏è Data vectorized\")\n",
    "\n",
    "    def computeSparsicity(self):\n",
    "        \"\"\"This function computes the sparsicity\n",
    "        \"\"\"\n",
    "        data_dense = self.vectorized.todense()\n",
    "        print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")\n",
    "\n",
    "    def buildLDAModel(self, hyper_parameters): \n",
    "        \"\"\"This function builds LDA model and calculates its log-likelihood and Perplexity\n",
    "        \"\"\"\n",
    "        self.spinner.start()\n",
    "        self.spinner.write(\"ü§ñ Model building\")\n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "                                      n_components= int(hyper_parameters['n_components']),               # Number of topics\n",
    "                                      max_iter= int(hyper_parameters['max_iter']),               # Max learning iterations\n",
    "                                      learning_method= str(hyper_parameters['learning_method']),   \n",
    "                                      random_state= int(hyper_parameters['random_state']),          # Random state\n",
    "                                      batch_size= int(hyper_parameters['batch_size']),            # n docs in each learning iter\n",
    "                                      evaluate_every = int(hyper_parameters['evaluate_every']),       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = int(hyper_parameters['n_jobs']),               # Use all available CPUs\n",
    "                                      doc_topic_prior = float(hyper_parameters['doc_topic_prior']),\n",
    "                                      learning_decay = float(hyper_parameters['learning_decay']),\n",
    "                                      topic_word_prior = float(hyper_parameters['topic_word_prior']),\n",
    "                                     )\n",
    "        lda_output = self.lda_model.fit_transform(self.vectorized)\n",
    "        self.spinner.stop()\n",
    "        # See model parameters\n",
    "        print('Model Parameters',self.lda_model.get_params())\n",
    "        # Log Likelyhood: Higher the better\n",
    "        print(\"Log Likelihood: \", self.lda_model.score(self.vectorized))\n",
    "        # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "        print(\"Perplexity: \", self.lda_model.perplexity(self.vectorized))\n",
    "        \n",
    "    def visualizeLDAvis(self):\n",
    "        \"\"\"This function generates the pyLDAvis report and saves it to the output folder\n",
    "        \"\"\"\n",
    "        panel = pyLDAvis.sklearn.prepare(self.lda_model, self.vectorized, self.vectorizer, mds='tsne')\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\LDA\\\\{self.dirName}\"\n",
    "        pyLDAvis.save_html(panel, os.path.join(complete_path + f'\\\\{self.dirName}_lda.html'))\n",
    "        self.spinner.write(f'‚úîÔ∏è Report saved')\n",
    "         \n",
    "    def buildImprovisedLDAModel(self, search_params): \n",
    "        \"\"\"This builds the optimized LDA model by using GridSearchCV\n",
    "        \"\"\"\n",
    "        self.spinner.start()\n",
    "        self.spinner.write('ü§ñ Building improvised model')\n",
    "        search_params = search_params\n",
    "        lda = LatentDirichletAllocation()\n",
    "        model = GridSearchCV(lda, param_grid=search_params)\n",
    "        model.fit(self.vectorized)\n",
    "        self.best_lda_model = model.best_estimator_\n",
    "        self.best_model_output = self.best_lda_model.fit_transform(self.vectorized)\n",
    "        self.spinner.stop()\n",
    "        print(\"Best Models Params: \", model.best_params_)\n",
    "        print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "        print(\"Model Perplexity: \", self.best_lda_model.perplexity(self.vectorized))\n",
    "        panel = pyLDAvis.sklearn.prepare(self.best_lda_model, self.vectorized, self.vectorizer, mds='tsne')\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\LDA\\\\{self.dirName}\"\n",
    "        pyLDAvis.save_html(panel, os.path.join(complete_path +  f'\\\\{self.dirName}_best_lda.html'))\n",
    "        self.spinner.write(f'‚úîÔ∏è Report saved')\n",
    "         \n",
    "    def wordsInTopics(self):\n",
    "        \"\"\"Display first 10 words in each topic\n",
    "        \"\"\"\n",
    "        print('First 10 words in each topic:')\n",
    "        featureNames = self.vectorizer.get_feature_names()\n",
    "        for idx, topic in enumerate(self.best_lda_model.components_):\n",
    "            print (\"Topic \", idx, \" \".join(featureNames[i] for i in topic.argsort()[:-10 - 1:-1]))       \n",
    "    \n",
    "    def calculateDominantTopic(self):\n",
    "        \"\"\"This function calculates which topic is dominant for each data point/row in the dataframe\n",
    "        \"\"\"\n",
    "        # Create Document - Topic Matrix\n",
    "        lda_output = self.best_lda_model.transform(self.vectorized)\n",
    "        topicnames = [\"Topic\" + str(i) for i in range(self.best_lda_model.n_components)]\n",
    "        docnames = [\"Doc\" + str(i) for i in range(len(self.data_frame))]\n",
    "        self.df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "        dominant_topic = np.argmax( self.df_document_topic.values, axis=1)\n",
    "        self.df_document_topic['dominant_topic'] = dominant_topic\n",
    "        self.data_frame['dominant_topic'] = dominant_topic\n",
    "        return self.data_frame.head(4)\n",
    "\n",
    "    def getTopicDistribution(self):\n",
    "        \"\"\"This function displays the distribution of data/rows/papers per topic\n",
    "        \"\"\"\n",
    "        self.df_topic_distribution = self.df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
    "        self.df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
    "        print('Topic distribution')\n",
    "        return self.df_topic_distribution.sort_values(by=['Topic Num'])\n",
    "\n",
    "    def topKeywordsInEachTopic(self, n_words=20):\n",
    "        \"\"\"This function displays top keywords in each topic\n",
    "\n",
    "        Args:\n",
    "            n_words (int, optional): Number of words you want to display. Defaults to 20.\n",
    "        \"\"\"\n",
    "        # Show top n keywords for each topic\n",
    "        keywords = np.array(self.vectorizer.get_feature_names())\n",
    "        topic_keywords = []\n",
    "        for topic_weights in self.best_lda_model.components_:\n",
    "            top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "            topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "        self.df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "        self.df_topic_keywords.columns = ['Word '+str(i) for i in range(self.df_topic_keywords.shape[1])] \n",
    "        self.df_topic_keywords['Topic'] = ['Topic '+str(i) for i in range(self.df_topic_keywords.shape[0])]\n",
    "        self.df_topic_keywords.set_index('Topic')\n",
    "        print(f'Top {n_words} words in each topic')\n",
    "        return self.df_topic_keywords\n",
    "\n",
    "    def printAbstractForTopic(self, topic=0):\n",
    "        \"\"\"This function prints the abstract for the given topic\n",
    "\n",
    "        Args:\n",
    "            topic (int, optional): Topic number for which you want to display the abstract. Defaults to 0.\n",
    "        \"\"\"\n",
    "        abstract = self.data_frame[self.data_frame.dominant_topic == topic].Abstract_clean\n",
    "        print(f'Abstract belonging to topic number {topic}')\n",
    "        return abstract\n",
    "\n",
    "    def topCitedTopics(self, year):\n",
    "        \"\"\"This function calculates the top cited topics according to total cites, topic age, paper count, cite per year and cite per topic\n",
    "        \"\"\"\n",
    "        cite_sum = []\n",
    "        topic_age = []\n",
    "\n",
    "        for i in range(self.best_lda_model.n_components):\n",
    "            group_rows = self.data_frame[self.data_frame.dominant_topic == i]\n",
    "            cite_sum.append(group_rows.Cites.sum())\n",
    "            topic_age.append((year - group_rows.Date.astype('datetime64[ns]').dt.year).sum())\n",
    "            \n",
    "        self.df_topic_distribution['Cite Sum'] = cite_sum\n",
    "        self.df_topic_distribution['Topic Age'] = topic_age\n",
    "        self.df_topic_distribution['Paper Count'] = self.df_topic_distribution['Num Documents']\n",
    "        self.df_topic_distribution['Cite Per Year'] = self.df_topic_distribution['Cite Sum'] / self.df_topic_distribution['Topic Age']\n",
    "        self.df_topic_distribution['Cite Per Topic'] = self.df_topic_distribution['Cite Sum'] / self.df_topic_distribution['Paper Count']\n",
    "\n",
    "        # Top cited per year\n",
    "        top_cited_per_year = self.df_topic_distribution[self.df_topic_distribution['Cite Per Year'] == self.df_topic_distribution['Cite Per Year'].max()]\n",
    "        print('Top cited per year')\n",
    "        print(self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(top_cited_per_year['Topic Num'].values[0])])\n",
    "\n",
    "        # Most cited\n",
    "        most_cited = self.df_topic_distribution[self.df_topic_distribution['Cite Sum'] == self.df_topic_distribution['Cite Sum'].max()]\n",
    "        print('Most cited')\n",
    "        print(self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(most_cited['Topic Num'].values[0])])\n",
    "\n",
    "        # Oldest topic\n",
    "        oldest_topic = self.df_topic_distribution[self.df_topic_distribution['Topic Age'] == self.df_topic_distribution['Topic Age'].max()]\n",
    "        print('Oldest topic')\n",
    "        print(self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(oldest_topic['Topic Num'].values[0])])\n",
    "\n",
    "        # Most popular topic\n",
    "        most_popular = self.df_topic_distribution[self.df_topic_distribution['Paper Count'] == self.df_topic_distribution['Paper Count'].max()]\n",
    "        self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(most_popular['Topic Num'].values[0])]\n",
    "\n",
    "    def getTopFive(self):\n",
    "        \"\"\"This function calculates the top five cited topics according to total cites, topic age, paper count, cite per year and cite per topic\n",
    "        \"\"\"\n",
    "        # Top 5 cited per year\n",
    "        sorted_cite_per_year = self.df_topic_distribution.sort_values(by='Cite Per Year', ascending=False)\n",
    "        top_five_topic_numbers = sorted_cite_per_year[:5]\n",
    "        print('Top 5 cited topics per year')\n",
    "        for index, row in top_five_topic_numbers.iterrows():\n",
    "            words = self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(int(row['Topic Num']))]\n",
    "            print(words)\n",
    "\n",
    "        # Top 5 most cited \n",
    "        sorted_cited = self.df_topic_distribution.sort_values(by='Cite Sum', ascending=False)\n",
    "        top_five_topic_numbers = sorted_cited[:5]\n",
    "        print('Top 5 Most cited topics')\n",
    "        for index, row in top_five_topic_numbers.iterrows():\n",
    "            words = self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(int(row['Topic Num']))]\n",
    "            print(words)\n",
    "\n",
    "        # Top 5 oldest topic\n",
    "        sorted_topic_age = self.df_topic_distribution.sort_values(by='Topic Age', ascending=False)\n",
    "        top_five_topic_numbers = sorted_topic_age[:5]\n",
    "        print('Top 5 Oldest topics')\n",
    "        for index, row in top_five_topic_numbers.iterrows():\n",
    "            words = self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(int(row['Topic Num']))]\n",
    "            print(words)\n",
    "\n",
    "        # Top 5 most popular\n",
    "        sorted_paper_count = self.df_topic_distribution.sort_values(by='Paper Count', ascending=False)\n",
    "        top_five_topic_numbers = sorted_paper_count[:5]\n",
    "        print('Top 5 most cited topics')\n",
    "        for index, row in top_five_topic_numbers.iterrows():\n",
    "            words = self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(int(row['Topic Num']))]\n",
    "            print(words)\n",
    "    \n",
    "    def hotAndColdTopicByDate(self):\n",
    "        medians = []\n",
    "        for i in range(self.best_lda_model.n_components):\n",
    "            group_rows = self.data_frame[self.data_frame.dominant_topic == i]\n",
    "            median = group_rows.Date.astype('datetime64[ns]').quantile(0.5, interpolation=\"midpoint\")\n",
    "            medians.append(median)\n",
    "        \n",
    "        median_dates = pd.DataFrame(medians, columns=['Date'])\n",
    "        median_dates['Date'].dt.date \n",
    "\n",
    "        self.hot = median_dates['Date'].idxmax() \n",
    "        hot_words = self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(self.hot)]\n",
    "        print('Hot Words')\n",
    "        print(hot_words)\n",
    "\n",
    "        hot_topics = self.data_frame[self.data_frame['dominant_topic'] == self.hot]\n",
    "        print('Hot topic titles:')\n",
    "        print(hot_topics.Title_clean)\n",
    "\n",
    "        self.cold = median_dates['Date'].idxmin() \n",
    "        cold_words = self.df_topic_keywords[self.df_topic_keywords.Topic == 'Topic '+str(self.cold)]\n",
    "        print('Cold Words')\n",
    "        print(cold_words)\n",
    "\n",
    "        cold_topics = self.data_frame[self.data_frame['dominant_topic'] == self.cold]\n",
    "        print('Cold topic titles')\n",
    "        print(cold_topics.Title_clean)\n",
    "   \n",
    "    def plotTopicTrend(self):\n",
    "        self.data_frame['Year'] = pd.DatetimeIndex(self.data_frame['Date']).year\n",
    " \n",
    "        topic_dictionaries = []\n",
    "\n",
    "        for i in range(self.best_lda_model.n_components):\n",
    "            group_rows = self.data_frame[self.data_frame.dominant_topic == i]\n",
    "            topic_years = group_rows.Year\n",
    "            topic_year_count = Counter(topic_years) \n",
    "            topic_dictionaries.append(topic_year_count)\n",
    "            \n",
    "        topic_trend = pd.DataFrame.from_dict(topic_dictionaries)\n",
    "        topic_trend.set_index(topic_trend.columns[0])\n",
    "        topic_trend.fillna(0, inplace=True) \n",
    "        topic_trend_transposed = topic_trend.T\n",
    "        topic_trend_transposed['Year'] = list(topic_trend.columns)\n",
    "        topic_trend_transposed.drop(['Year'], axis=1, inplace=True)\n",
    "        topic_trend_transposed.sort_index(inplace=True)\n",
    "        ax = topic_trend_transposed.plot(figsize=(20, 10), title='Topic trends')\n",
    "        ax.set_xticklabels(topic_trend_transposed.index)\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\LDA\\\\{self.dirName}\"\n",
    "        ax.get_figure().savefig(os.path.join(complete_path, f\"{self.dirName}_topic_trends.png\"))\n",
    "        self.spinner.write(f'‚úîÔ∏è Figure saved')\n",
    "\n",
    "    def plotHotVsCold(self):\n",
    "        hot_topic_data = self.data_frame[self.data_frame['dominant_topic'] == self.hot]\n",
    "        hot_topic_data_years =  hot_topic_data.Year\n",
    "        hot_topic_year_count = [Counter(hot_topic_data_years)] \n",
    "        hot_topic_year_count=  pd.DataFrame.from_dict(hot_topic_year_count)\n",
    "        hot_topic_year_count['Type'] = 'Hot topic'\n",
    "        \n",
    "        cold_topic_data = self.data_frame[self.data_frame['dominant_topic'] == self.cold]\n",
    "        cold_topic_data_years =  cold_topic_data.Year\n",
    "        cold_topic_year_count = [Counter(cold_topic_data_years)] \n",
    "        cold_topic_year_count=  pd.DataFrame.from_dict(cold_topic_year_count)\n",
    "        cold_topic_year_count['Type'] = 'Cold Topic'\n",
    "\n",
    "        combined = pd.concat([hot_topic_year_count, cold_topic_year_count], ignore_index=True)\n",
    "        combined.fillna(0, inplace=True)\n",
    "        \n",
    "        combined_trasnposed = combined.T \n",
    "        combined_trasnposed.rename(columns={0: \"Hot Topic\", 1: \"Cold Topic\"}, inplace=True)\n",
    "        combined_trasnposed.drop(['Type'], axis=0, inplace=True)\n",
    "        combined_trasnposed.sort_index(inplace=True)\n",
    "        \n",
    "        ax = combined_trasnposed.plot(figsize=(20, 10), title='Hot vs Cold')\n",
    "        ax.set_xticklabels(combined_trasnposed.index)\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\LDA\\\\{self.dirName}\"\n",
    "        ax.get_figure().savefig(os.path.join(complete_path, f\"{self.dirName}_hot_vs_cold.png\"))\n",
    "        self.spinner.write(f'‚úîÔ∏è Figure saved')\n",
    "\n",
    "    def trendAnalysisUsingTheta(self):\n",
    "        theta = self.best_model_output\n",
    "        years = pd.DatetimeIndex(self.data_frame['Date']).year\n",
    "        theta_df = pd.DataFrame(theta)\n",
    "        theta_df['Years'] = years\n",
    "\n",
    "        unique_years = theta_df['Years'].unique()\n",
    "        theta_mean_by_year = []\n",
    "\n",
    "        for i in range(len(unique_years)):\n",
    "            grouped_thetas = theta_df[theta_df['Years'] == unique_years[i]]\n",
    "            theta_mean_by_year.append(grouped_thetas.mean())\n",
    "            \n",
    "        theta_mean_by_year = pd.DataFrame(theta_mean_by_year)\n",
    "        x = theta_mean_by_year['Years']\n",
    "\n",
    "        cols = theta_mean_by_year.drop(['Years'], axis=1).columns\n",
    "\n",
    "        model_details = []\n",
    "\n",
    "        for index, value in enumerate(cols):\n",
    "            y = theta_mean_by_year[value]\n",
    "            est = sma.OLS(y, x)\n",
    "            fitted_model = est.fit()\n",
    "            details = {\n",
    "                'topic' : value,\n",
    "                'pvalue' : fitted_model.pvalues[0],\n",
    "                'coef' : fitted_model.params[0]\n",
    "            }\n",
    "            model_details.append(details)\n",
    "            \n",
    "        model_details_df = pd.DataFrame.from_dict(model_details)  \n",
    "        positive_slope = model_details_df[model_details_df['coef'] >=0]\n",
    "        negative_slope = model_details_df[model_details_df['coef'] <0]\n",
    "        print(positive_slope.shape, negative_slope.shape)\n",
    "\n",
    "        p_level = [0.01, 0.03, 0.05]\n",
    "        trends = []\n",
    "\n",
    "        for i in range(len(p_level)):\n",
    "            positive_group = positive_slope[positive_slope['pvalue'] <= p_level[i]]\n",
    "            negative_group = negative_slope[negative_slope['pvalue'] <= p_level[i]]\n",
    "            count_pos = len(positive_group)\n",
    "            count_neg = len(negative_group)\n",
    "            data = {\n",
    "                'P-level' : p_level[i],\n",
    "                'Negative Trend': count_neg,\n",
    "                'Positive Trend' : count_pos,\n",
    "                'Hot Topics' : positive_group.topic.values,\n",
    "                'Cold Topics' : negative_group.topic.values\n",
    "                \n",
    "            }\n",
    "            trends.append(data)\n",
    "            \n",
    "        trends = pd.DataFrame(trends)\n",
    "        \n",
    "        thetas_by_year = theta_mean_by_year \n",
    "        thetas_by_year.sort_values(by='Years',inplace=True)\n",
    "\n",
    "        hot_topics =  list(trends[trends['P-level'] == 0.05]['Hot Topics'])\n",
    "        cold_topics =  list(trends[trends['P-level'] == 0.05]['Cold Topics']) \n",
    "\n",
    "        hot_topic_trend = thetas_by_year[hot_topics[0]]\n",
    "        if  hot_topic_trend.shape[1] > 0:\n",
    "            hot_topic_trend['Years'] = theta_mean_by_year.Years\n",
    "            \n",
    "        cold_topic_trend = thetas_by_year[cold_topics[0]]\n",
    "        if  cold_topic_trend.shape[1] > 0:\n",
    "            cold_topic_trend['Years'] = theta_mean_by_year.Years\n",
    "\n",
    "\n",
    "        if hot_topic_trend.shape[1] > 0:\n",
    "            ax = hot_topic_trend.plot(x='Years',figsize=(20, 10))\n",
    "            ax.set_xticklabels(theta_mean_by_year.Years)\n",
    "            complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\LDA\\\\{self.dirName}\"\n",
    "            ax.get_figure().savefig(os.path.join(complete_path, f\"{self.dirName}_hot_based_on_theta.png\"))\n",
    "            self.spinner.write(f'‚úîÔ∏è Figure saved')\n",
    "        else: \n",
    "            print('No hot topic')\n",
    "\n",
    "        \n",
    "        if cold_topic_trend.shape[1] > 0:\n",
    "            ax = cold_topic_trend.plot(x='Years',figsize=(20, 10))\n",
    "            ax.set_xticklabels(cold_topic_trend.Years)\n",
    "            complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\LDA\\\\{self.dirName}\"\n",
    "            ax.get_figure().savefig(os.path.join(complete_path, f\"{self.dirName}_cold_based_on_theta.png\"))\n",
    "            self.spinner.write(f'‚úîÔ∏è Figure saved')\n",
    "            \n",
    "        else: \n",
    "            print('No cold topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed089f27",
   "metadata": {},
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5354523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = readFile(config['REDDIT_DATA_CSV'], config['STORAGE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda = LDA(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.createOutputDir('Reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47210be",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.mergeTokenizedData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.lemmatization(allowed_postags=LDAConfig['ALLOWED_POSTAGS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.vectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.computeSparsicity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.buildLDAModel(LDAConfig['MODEL_HYPER_PARAMETERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37323ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.visualizeLDAvis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.buildImprovisedLDAModel(LDAConfig['SEARCH_PARAMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4919c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.wordsInTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.calculateDominantTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813e0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.getTopicDistribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcd24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.topKeywordsInEachTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a5996",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.printAbstractForTopic(LDAConfig['ABASTRACT_FOR_TOPIC_NUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c772a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.topCitedTopics(LDAConfig['YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df60ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.getTopFive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aca0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.hotAndColdTopicByDate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde81ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.plotTopicTrend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.plotHotVsCold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5487ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_lda.trendAnalysisUsingTheta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b0999",
   "metadata": {},
   "source": [
    "### Stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed98ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data = readFile(config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda = LDA(stackoverflow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72262db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.createOutputDir('Stackoverflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.mergeTokenizedData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.lemmatization(allowed_postags=LDAConfig['ALLOWED_POSTAGS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b848d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.vectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947861ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.computeSparsicity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b3ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.buildLDAModel(LDAConfig['MODEL_HYPER_PARAMETERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a9821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.visualizeLDAvis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2370e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.buildImprovisedLDAModel(LDAConfig['SEARCH_PARAMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc4fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.wordsInTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.calculateDominantTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7acb015",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.getTopicDistribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f358c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.topKeywordsInEachTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77521af",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.printAbstractForTopic(LDAConfig['ABASTRACT_FOR_TOPIC_NUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8862cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.topCitedTopics(LDAConfig['YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056b1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.getTopFive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d58053",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.hotAndColdTopicByDate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.plotTopicTrend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d63a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.plotHotVsCold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_lda.trendAnalysisUsingTheta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2195f",
   "metadata": {},
   "source": [
    "### Scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data = readFile(config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12331e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda = LDA(scopus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b602e888",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.createOutputDir('Scopus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.mergeTokenizedData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58aea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.lemmatization(allowed_postags=LDAConfig['ALLOWED_POSTAGS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea82532",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.vectorization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf181611",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.computeSparsicity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a692c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.buildLDAModel(LDAConfig['MODEL_HYPER_PARAMETERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d38de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.visualizeLDAvis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.buildImprovisedLDAModel(LDAConfig['SEARCH_PARAMS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.wordsInTopics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8485c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.calculateDominantTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.getTopicDistribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.topKeywordsInEachTopic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.printAbstractForTopic(LDAConfig['ABASTRACT_FOR_TOPIC_NUMBER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c99a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.topCitedTopics(LDAConfig['YEAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3620a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.getTopFive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b5c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.hotAndColdTopicByDate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc44d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.plotTopicTrend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.plotHotVsCold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_lda.trendAnalysisUsingTheta()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
