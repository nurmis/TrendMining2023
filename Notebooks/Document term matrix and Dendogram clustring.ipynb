{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e511c60",
   "metadata": {},
   "source": [
    "# Trend Mining "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c3e40",
   "metadata": {},
   "source": [
    "## Document term matrix and Dendogram clustring\n",
    "\n",
    "In this notebook you will be able to analyze Document term matrix and make Dendogram clustring. \n",
    "- Configurations for this notebook can be found in **Dendogram.yaml** file inside the **Config** folder\n",
    "- Make sure you follow the setup instructions on **Readme.md** and have installed all the packages required for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd66e48",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import yaml\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from yaspin import yaspin\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords \n",
    "from yaml.loader import SafeLoader\n",
    "from nltk.stem import PorterStemmer\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.spatial.distance import pdist  \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef74a79",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a08bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Config/Miners.yaml') as f:\n",
    "    config = yaml.load(f, Loader=SafeLoader)\n",
    "print('General Config:', config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Config/Dendogram.yaml') as f:\n",
    "    dendogramConfig = yaml.load(f, Loader=SafeLoader)\n",
    "print('Dendogram Config:', dendogramConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d61e6",
   "metadata": {},
   "source": [
    "### Common Functions and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2591af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(file, path):\n",
    "    try:\n",
    "        spinner = yaspin()\n",
    "        complete_path = f'{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{path}\\\\{file}'\n",
    "        file_data = pd.read_csv(complete_path, index_col=0)\n",
    "        spinner.write(\"‚úîÔ∏è File loaded.\")\n",
    "        spinner.stop()\n",
    "        return file_data\n",
    "    except Exception as e:\n",
    "        print('Error reading file',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59de31",
   "metadata": {},
   "source": [
    "##### Common Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c3703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTM():\n",
    "    \"\"\"\n",
    "    This is the class implementation for generating Document-Term Matrix and Dendogram clustring\n",
    "  \"\"\"\n",
    "  \n",
    "    def __init__(self, data_frame):\n",
    "        self.data_frame = data_frame\n",
    "        self.vec_df = pd.DataFrame()\n",
    "        self.frequent_words = pd.DataFrame()\n",
    "        self.sorted_frequent_words = pd.DataFrame()\n",
    "        self.top_words = pd.DataFrame()\n",
    "        self.dirName = \"\"\n",
    "        self.spinner = yaspin()\n",
    "    \n",
    "        print(f'Data has {len(data_frame)} rows')\n",
    "        \n",
    "        \n",
    "    def createOutputDir(self, dirName):\n",
    "        \"\"\"This function creates the folder to store the output graphs and images\n",
    "\n",
    "        Args:\n",
    "            dirName (str): Name of the output folder\n",
    "        \"\"\"\n",
    "        self.dirName = dirName\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\Dendogram\\\\{self.dirName}\"\n",
    "        does_folder_exist = os.path.exists(complete_path)\n",
    "        if (does_folder_exist):\n",
    "            self.spinner.write(\"‚úîÔ∏è Output directory already exists.\")\n",
    "        else:\n",
    "            os.makedirs(complete_path)\n",
    "            self.spinner.write(\"‚úîÔ∏è Folder created for output storage\")\n",
    "            \n",
    "\n",
    "    def saveFile(self, filename, path):\n",
    "        \"\"\"This function saves the file with all new columns\n",
    "\n",
    "        Args:\n",
    "            file (str): file name\n",
    "            path (str): file path\n",
    "        \"\"\"\n",
    "        complete_path = f'{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{path}'\n",
    "        if(os.path.exists(f'{complete_path}\\\\{filename}')):\n",
    "            self.spinner.write(f\"üîÅ Replacing already existing {filename} file\")\n",
    "            os.remove(f'{complete_path}\\\\{filename}')\n",
    "        \n",
    "        self.data_frame.to_csv(f'{complete_path}\\\\{filename}')\n",
    "        print()\n",
    "        self.spinner.write(f'‚úîÔ∏è {filename} saved in {path} directory')\n",
    "        \n",
    "        \n",
    "    def get_data(self):\n",
    "        \"\"\"This function returns the dataframe itself\n",
    "\n",
    "        Returns:\n",
    "            dataframe: data that is operated upon\n",
    "        \"\"\"\n",
    "        return self.data_frame\n",
    "  \n",
    "\n",
    "    def print_data_head(self, rows=3):\n",
    "        \"\"\"This function prints the top rows of the data\n",
    "\n",
    "        Args:\n",
    "            rows (int, optional): number of rows from dataset you want to print. Defaults to 3.\n",
    "        \"\"\"\n",
    "        print(\"Data head with top\", rows, \"rows\")\n",
    "        print(self.data_frame.head(rows))\n",
    "\n",
    "        \n",
    "    def print_data_tail(self, rows=3):\n",
    "        \"\"\"This function prints last rows of the data\n",
    "\n",
    "        Args:\n",
    "            rows (int, optional): number of rows from dataset you want to print. Defaults to 3.\n",
    "        \"\"\"\n",
    "        print(\"Data tail with last\", rows, \"rows\")\n",
    "        print(self.data_frame.tail(rows))\n",
    "\n",
    "        \n",
    "    def print_dtm(self, rows=3):\n",
    "        \"\"\"This function prints the vectorized data\n",
    "\n",
    "        Args:\n",
    "        rows (int, optional): number of rows from vectorized data you want to print. Defaults to 3.\n",
    "        \"\"\"\n",
    "        print(\"Vectorized data with top\", rows, \"rows\")\n",
    "        print(self.vec_df.head(rows))\n",
    "\n",
    "        \n",
    "    def print_frequent_words(self,rows=3):\n",
    "        \"\"\"This function prints the most frequent words\n",
    "\n",
    "        Args:\n",
    "            rows (int, optional): number of rows to be printed. Defaults to 3.\n",
    "        \"\"\"\n",
    "        print(\"Frequent top\", rows, \"rows\")\n",
    "        print(self.frequent_words.head(rows))\n",
    "\n",
    "        \n",
    "    def print_sorted_frequent_words(self, rows=3):\n",
    "        \"\"\"This function prints the frequent words in sorted order\n",
    "\n",
    "        Args:\n",
    "            rows (int, optional): number of rows to be printed. Defaults to 3.\n",
    "        \"\"\"\n",
    "        print(f'Top {rows} most frequent words:')\n",
    "        self.sorted_frequent_words.set_index('word')\n",
    "        print (self.sorted_frequent_words.head(rows))  \n",
    "  \n",
    "\n",
    "    def print_top_words(self, rows=3):\n",
    "        \"\"\"This function prints the   to top words\n",
    "\n",
    "        Args:\n",
    "            rows (int, optional): number of rows to be printed. Defaults to 3.\n",
    "        \"\"\"\n",
    "        print(\"Top\", rows, \"words\")\n",
    "        print(self.top_words.head(rows))\n",
    "\n",
    "        \n",
    "    def remove_stop_words(self, custom_stopwords = [] ):\n",
    "        \"\"\"This function is used to remove the stop words\n",
    "\n",
    "        Args:\n",
    "            custom_stopwords (list, optional): any other custom stop word. Defaults to [].\n",
    "\n",
    "        Returns:\n",
    "            dataframe: dataframe with removed stop words in abstract and in title \n",
    "        \"\"\"\n",
    "        try:\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "            stop_words = stop_words.union(custom_stopwords)\n",
    "            print('total stop words:', len(stop_words))\n",
    "            self.data_frame['Abstrat_without_stopwords'] = self.data_frame['Abstract_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "            self.data_frame['Title_without_stopwords'] = self.data_frame['Title_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "            self.spinner.write(f'‚úîÔ∏è Stop words removed')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "            \n",
    "    def combine_title_and_abs(self):\n",
    "        \"\"\"This function combines the title and abstract with no stop words\n",
    "\n",
    "        Returns:\n",
    "            dataframe: dataframe with merged title and abstract in a new column\n",
    "        \"\"\"\n",
    "        self.data_frame['Merged_title_and_abs'] = self.data_frame[\"Title_without_stopwords\"] + self.data_frame[\"Abstrat_without_stopwords\"]\n",
    "        self.spinner.write(f'‚úîÔ∏è Data combined')\n",
    "\n",
    "    \n",
    "    def stemming(self):\n",
    "        \"\"\"This function is used to stem and tokenize the data\n",
    "\n",
    "        Returns:\n",
    "            dataframe: dataframe with tokenized and stemmed data\n",
    "        \"\"\"\n",
    "        porter_stemmer = PorterStemmer() \n",
    "        self.data_frame['Tokenized_data'] = self.data_frame.apply(lambda row: nltk.word_tokenize(row['Merged_title_and_abs']), axis=1)\n",
    "        self.data_frame['Stem_data'] = self.data_frame['Tokenized_data'].apply(lambda x : [porter_stemmer.stem(y) for y in x])\n",
    "        self.spinner.write(f'‚úîÔ∏è Stemming applied to data')\n",
    "\n",
    "    \n",
    "    def document_term_matrix(self, column_name):\n",
    "        \"\"\"This function generated document term matrix\n",
    "\n",
    "        Args:\n",
    "            column_name (str): column of the dataframe to which this function is applied\n",
    "        \"\"\"\n",
    "        vec = CountVectorizer()\n",
    "        stem_data = self.data_frame .apply(lambda row : ' '.join(row[column_name]), axis=1)\n",
    "        stem_data  = stem_data.tolist()\n",
    "        X = vec.fit_transform(stem_data)\n",
    "        self.vec_df = pd.DataFrame(X.toarray(), columns = vec.get_feature_names())\n",
    "        self.spinner.write(f'‚úîÔ∏è Document term matrix created for {column_name}')\n",
    "\n",
    "\n",
    "    def frequent_terms(self): \n",
    "        \"\"\"This function is used to get frequent terms\n",
    "        \"\"\"\n",
    "        vec_df = self.vec_df\n",
    "        self.frequent_words['word'] = vec_df.columns\n",
    "        self.frequent_words['frequency'] = list(vec_df.sum())\n",
    "        self.spinner.write(f'‚úîÔ∏è Frequent words calculated')\n",
    "\n",
    "        \n",
    "    def sort_frequent_terms(self):\n",
    "        \"\"\"This function sorts the frequent terms based on frequency\n",
    "        \"\"\"\n",
    "        self.sorted_frequent_words = pd.DataFrame(columns=['word', 'frequency'])\n",
    "        self.sorted_frequent_words = self.frequent_words.sort_values(by=['frequency'], ascending=False)\n",
    "        self.spinner.write(f'‚úîÔ∏è Frequent word sorted')\n",
    "        \n",
    "    def keep_top_words(self, max_frequency=100): \n",
    "        \"\"\"This function keeps top words based on the max_frequency\n",
    "\n",
    "        Args:\n",
    "            max_frequency (int, optional): frequency threshold. Defaults to 100.\n",
    "        \"\"\"\n",
    "        self.top_words = self.sorted_frequent_words[self.sorted_frequent_words['frequency'] >= max_frequency]\n",
    "        self.spinner.write(f'‚úîÔ∏è Top {max_frequency} words kept')\n",
    "        \n",
    "    def visualize_frequent_words(self):\n",
    "        \"\"\"Saves the frequent words to an image\n",
    "        \"\"\"\n",
    "        self.spinner.write(f'‚úîÔ∏è Figure saved')\n",
    "        plt.rcParams[\"figure.figsize\"] = 20,40\n",
    "        sns.barplot(x=\"frequency\", y=\"word\", data=self.top_words)\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\Dendogram\\\\{self.dirName}\"\n",
    "        plt.savefig(os.path.join(complete_path, f\"{self.dirName}_frequent_terms.png\"))\n",
    "\n",
    "    def dendogram_clusting(self):\n",
    "        \"\"\"Generates and saves dendogram to an image\n",
    "        \"\"\"\n",
    "        self.spinner.start()\n",
    "        self.spinner.write(f'‚ú® Generating dendogram cluster')        \n",
    "        distance_matrix = pdist(self.vec_df, metric='euclidean')\n",
    "        plt.figure(figsize=(25, 200))\n",
    "        plt.title('Hierarchical Clustering Dendrogram') \n",
    "        dendrogram = sch.dendrogram(sch.linkage(distance_matrix, method = 'ward'),\n",
    "                                orientation=\"right\", \n",
    "                                labels=self.data_frame['Title_without_stopwords'].tolist(),\n",
    "                                leaf_font_size=9\n",
    "                                )\n",
    "        self.spinner.stop()\n",
    "        complete_path = f\"{os.path.dirname(os.path.abspath(os.getcwd()))}\\\\{config['OUTPUT_PATH']}\\\\Dendogram\\\\{self.dirName}\"\n",
    "        plt.savefig(os.path.join(complete_path, f\"{self.dirName}_dendogram.png\"))\n",
    "        self.spinner.write(f'‚úîÔ∏è Figure saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4039ed9",
   "metadata": {},
   "source": [
    "### Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = readFile(config['REDDIT_DATA_CSV'], config['STORAGE_PATH'])\n",
    "reddit_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bfd762",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM = DTM(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.createOutputDir(\"Reddit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d643dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.remove_stop_words(dendogramConfig['REDDIT_STOPWORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.combine_title_and_abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.stemming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5814c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.document_term_matrix(dendogramConfig['COLUMN_TO_TOKENIZE']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1515b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.frequent_terms()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.print_frequent_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a875999",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.sort_frequent_terms()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e75bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.print_sorted_frequent_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.keep_top_words(dendogramConfig['TOP_WORDS_MAX_FREQUENCY'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ddea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.print_top_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.visualize_frequent_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439de804",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.dendogram_clusting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data_DTM.saveFile(config['REDDIT_DATA_CSV'], config['STORAGE_PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4abb6e",
   "metadata": {},
   "source": [
    "### Stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502da935",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data = readFile(config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'])\n",
    "stackoverflow_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282182e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM = DTM(stackoverflow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f04b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.createOutputDir(\"Stackoverflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c963f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.remove_stop_words(dendogramConfig['STACKOVERFLOW_STOPWORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.combine_title_and_abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f4f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.stemming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.document_term_matrix(dendogramConfig['COLUMN_TO_TOKENIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf732a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.frequent_terms()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064fd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.print_frequent_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.sort_frequent_terms()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.print_sorted_frequent_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d51eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.keep_top_words(dendogramConfig['TOP_WORDS_MAX_FREQUENCY'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.print_top_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.visualize_frequent_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61717e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.dendogram_clusting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackoverflow_data_DTM.saveFile(config['STACKOVERFLOW_DATA_CSV'], config['STORAGE_PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d70e0f",
   "metadata": {},
   "source": [
    "### Scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b5ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data = readFile(config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'])\n",
    "scopus_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0129ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM = DTM(scopus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.createOutputDir(\"Scopus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35833b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.remove_stop_words(dendogramConfig['SCOPUS_STOPWORDS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52880bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.combine_title_and_abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cb0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.stemming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34208e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.document_term_matrix(dendogramConfig['COLUMN_TO_TOKENIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adf87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.frequent_terms()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.print_frequent_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79636ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.sort_frequent_terms()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.print_sorted_frequent_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bacf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.keep_top_words(dendogramConfig['TOP_WORDS_MAX_FREQUENCY'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2b7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.print_top_words(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dece168",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.visualize_frequent_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f563d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.dendogram_clusting() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc59570",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_data_DTM.saveFile(config['SCOPUS_DATA_CSV'], config['STORAGE_PATH'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
